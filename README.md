# üìö Personalized News Aggregator

An intelligent, AI-powered news aggregator that fetches real-time articles based on user queries and preferences, summarizes them using **RAG** (Retrieval-Augmented Generation), and filters them by region, language, and content safety.

---

## üß† Features

- ‚úÖ AI Summarization using LLM  
- ‚úÖ Retrieval-Augmented Generation (RAG) pipeline  
- ‚úÖ Full user authentication (Login/Register)  
- ‚úÖ News filtering by country/region & language  
- ‚úÖ Summaries free from headline duplication  
- ‚úÖ Safe content filtering (NSFW / inappropriate detection)  
- ‚úÖ Responsive Streamlit UI with pagination  
- ‚úÖ Personalized topic suggestions  
- ‚úÖ Full backend/frontend integration with Ollama + local vector DB  

---

## üóÇÔ∏è Project Structure

```text
project-root/
‚îú‚îÄ‚îÄ frontend/              # Streamlit frontend app
‚îÇ   ‚îú‚îÄ‚îÄ app.py             # Main Streamlit app
‚îÇ   ‚îú‚îÄ‚îÄ auth_client.py     # Handles register/login API calls
‚îÇ   ‚îú‚îÄ‚îÄ api_client.py      # Talks to backend to get news and summaries
‚îÇ   ‚îú‚îÄ‚îÄ topics.py          # Suggests topics based on query/articles
‚îÇ   ‚îú‚îÄ‚îÄ safety.py          # Simple client-side content safety check
‚îÇ   ‚îú‚îÄ‚îÄ ui.py              # Streamlit UI components (summary, articles)
‚îÇ   ‚îú‚îÄ‚îÄ settings.py        # Configurable frontend constants
‚îú‚îÄ‚îÄ backend/               # Flask or FastAPI backend (assumed)
‚îÇ   ‚îî‚îÄ‚îÄ ...                # Handles API routing, vector DB, summarization
‚îú‚îÄ‚îÄ app/                   # Core app logic
‚îÇ   ‚îú‚îÄ‚îÄ rag.py             # Full RAG pipeline: index ‚Üí retrieve ‚Üí generate
‚îÇ   ‚îú‚îÄ‚îÄ content_extractor.py
‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py
‚îÇ   ‚îî‚îÄ‚îÄ ...
```

## Setup Backend

The backend is responsible for fetching full articles, vector indexing, and calling the LLM.

```bash
uvicorn main:app --reload
streamlit run app.py
```

## üîê Authentication

- Users can register and log in.  
- Auth token is stored in session state.  
- Logged-in users get personalized suggestions.  

---

## üåç Country & Language Filtering

- Choose a country from the sidebar.  
- Region (`us`, `in`, `de`, etc.) and language (`en`, `de`, etc.) are auto-set.  
- When **Filter by my country** is enabled, all searches will respect the selected region/lang.  

---

## üß† How Summarization Works

### üîπ MAP Phase
For each article:
- Title + fulltext is passed to the LLM using a map prompt.  
- Extracts **3‚Äì5 key bullet points** (no quotes, no duplication from headline).  

### üîπ REDUCE Phase
- Combines all bullet points into a **user-personalized summary block**.  
- JSON output includes:
  - Overall summary  
  - Highlights  
  - Top articles with link  

---

## ‚ú® Smart Display Features

- üéØ Only shows **3 articles per page**  
- ‚úÖ Each article is summarized below the title  
- üìù Truncation is avoided; summary block has expanded height  
- üåê Summaries only shown in **English**, regardless of source article language  
- üõ°Ô∏è NSFW/unsafe content is filtered using a basic client-side check  

---

## üß± Backend API Structure
- POST /api/news         # get_news()
- POST /api/summarize    # summarize_batch()
- POST /api/register     # register()
- POST /api/login        # login()

## üöÄ Tech Stack & Tools

### üß± Frameworks & Libraries
| Layer         | Tech Used                                         |
| ------------- | ------------------------------------------------- |
| Frontend      | [Streamlit](https://streamlit.io/)                |
| Backend       | [FastAPI](https://fastapi.tiangolo.com/)          |
| API Handling  | `requests`, `httpx`                               |
| Auth          | Custom auth via FastAPI + JWT                     |
| Vector DB     | In-memory (or `ChromaDB`, optional)               |
| LLM Interface | [Ollama](https://ollama.com) (local)              |

---

### ü§ñ Large Language Models (LLMs)
| Model     | Provider | Purpose                  |
| --------- | -------- | ------------------------ |
| `mistral` | Ollama   | Main summarization model |
| `llama3`  | Ollama   | Alternative LLM option   |

**Used for:**
- Summarizing individual articles (**MAP**)
- Generating overall summary blocks (**REDUCE**)
- Removing duplication between headlines and content
- Ensuring English-only responses

---

### üß© Embeddings
| Component       | Value                                                      |
| --------------- | ---------------------------------------------------------- |
| Model           | `nomic-embed-text`                                         |
| Provider        | Ollama (via `embed_model`)                                 |
| Chunking Method | Fixed length (900 chars) with 150 char overlap             |
| Usage           | Vector similarity for retrieval during summarization (RAG) |

---

## üîê Authentication Flow

1. User registers or logs in  
2. Backend returns `user_id` and `token`  
3. Token is stored in `st.session_state`  
4. All backend requests include this token  
5. Search history and topic suggestions are tied to `user_id`  

## üîÑ Flow Diagram

```text
User Query
   ‚Üì
Fetch News Articles (Region + Lang)
   ‚Üì
Full Article ‚Üí Chunk ‚Üí Embed ‚Üí Vector DB
   ‚Üì
Query ‚Üí Similar Chunks Retrieved
   ‚Üì
MAP: Summarize Each Chunk
   ‚Üì
REDUCE: Combine Summaries + Generate Highlights
   ‚Üì
Display Summary + Top Articles in UI
```
---
## üß™ Example Prompts (LLM)
### MAP Prompt
```text
You are a concise journalist and summarizer. From the ARTICLE below, write 3‚Äì5 bullet points summarizing key insights.

Rules:
- Summarize in **English** only.
- Do NOT repeat phrases or facts from the article title or link.
- Do NOT quote directly or use exact sentences from the article.
- Avoid generic openings. Be specific and insightful.
- Each bullet must be >= 100 words.
- It should be concise and also informative.
- Do not begin with the acronyms

```
### REDUCE Prompt
User preferences: {prefs}
Query: {query}
Synthesize the per-article bullets into a briefing.

Return JSON:
{"summary":"...", "highlights":[ "...", "..."], "top":[{"title":"...","link":"..."}]}
Per-article bullets:
{bullets}
Only return JSON.

## üß† LLM and Embeddings Used

### üîÆ Large Language Model (LLM)

- **Model**: `mistral:latest`
- **Provider**: [Ollama](https://ollama.com/)
- **Local API Endpoint**: `http://localhost:11434/api/generate`
- **Used In**: `rag.py` via `_ollama_generate()` function
- **Purpose**:
  - Generate 3‚Äì5 bullet summaries per article (`MAP_PROMPT`)
  - Condense multiple article summaries into a personalized summary (`REDUCE_PROMPT`)

> Mistral is an efficient open-weight model that offers fast and accurate performance for summarization tasks. It's served locally via Ollama for low-latency inference.

---

### üß† Embeddings

- **Model**: `nomic-embed-text-v1`
- **Provider**: [Nomic AI](https://huggingface.co/nomic-ai/nomic-embed-text-v1)
- **Used In**: `vector_store.py` and `rag.py`
- **Purpose**:
  - Generate dense vector representations of article content chunks
  - Perform similarity-based retrieval of relevant articles from the vector store

---

### üóÇÔ∏è Vector Store

- **Library**: `ChromaDB`
- **Persistent Location**: `database/chroma/`
- **Used In**: `vector_store.py`
- **Purpose**:
  - Store vector embeddings of articles by `user_id`, `link`, and `chunk`
  - Query semantically relevant content for user queries
---

These components form the core of the Retrieval-Augmented Generation (RAG) pipeline that powers the intelligent summarization and personalized content delivery.

## üé• Watch the Demo

[![Watch the video](assets/app.png)](https://youtu.be/UvqC-XXiFVE?si=h6UDRc-mNQ0VrChR)
